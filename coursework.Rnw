\documentclass[10pt]{article}         % the type of document and font size (default 10pt)
\usepackage[noae]{Sweave} 
\usepackage{arev}
\usepackage[margin=1.0in]{geometry}   % sets all margins to 1in, can be changed
\usepackage{moreverb}                 % for verbatimtabinput -- LaTeX environment
\usepackage{url}                      % for \url{} command
\usepackage{amssymb}                  % for many mathematical symbols
\usepackage[pdftex]{lscape}           % for landscaped tables
\usepackage{longtable}                % for tables that break over multiple pages
%\usepackage{caret}
\usepackage{float}

\title{\Huge Title of Coursework \\[0.5in] \large CM3111: Big Data Analysis Coursework \\[2in]}  % to specify title

\author{Eilidh Southren \\[0.25in] Student ID: 1513195\\[3in]}          % to specify author(s)

\date{November 2018}
\begin{document}                      % document begins here
\SweaveOpts{concordance=TRUE}
\SweaveSyntax{SweaveSyntaxNoweb}

% If .nw file contains graphs: To specify that EPS/PDF graph files are to be 
% saved to 'graphics' sub-folder
%     NOTE: 'graphics' sub-folder must exist prior to Sweave step
%\SweaveOpts{prefix.string=graphics/plot}

% If .nw file contains graphs: to modify (shrink/enlarge} size of graphics 
% file inserted
%         NOTE: can be specified/modified before any graph chunk
\setkeys{Gin}{width=1.0\textwidth}

\maketitle              % makes the title
\pagebreak
\topskip0pt
\vspace*{\fill}


\begin{center}{How do you tell poisonous mushrooms from edible ones?  \\[0.25in]You give them to your little brother to eat first.}\end{center}
\vspace*{\fill}
\pagebreak

\tableofcontents        % inserts TOC (section, sub-section, etc numbers and titles)
%\listoftables           % inserts LOT (numbers and captions)
%\listoffigures          % inserts LOF (numbers and captions)
%                        %     NOTE: graph chunk must be wrapped with \begin{figure}, 
%                        %  \end{figure}, and \caption{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Where everything else goes

\pagebreak

\textbf{Introduction\\}
In this report I will detail my choice of dataset and the process of creating a model that fits the dataset.
\section{Data Exploration}
\subsection{Dataset Choice}
%For the dataset I opted to use the Mushroom Classification dataset provided by UCI Machine Learning on Kaggle\footnote{\href{https://www.kaggle.com/uciml/mushroom-classification/}{https://www.kaggle.com/uciml/mushroom-classification/}}. I chose this dataset because of its potential practical application in predicting the edibility of a mushroom merely based on characteristics without needing to know the name, merely by describing it can it be identified. This dataset was also featured on Kaggle and came very well recommended. The size of the dataset also made it a suitable choice it was a manageable size with a high variance in the attributes.
\subsection{Technology Platform}
The use of big data exploration tools such as Hadoop seemed to be of little merit as the size of this dataset does not warrant it. The source csv is a mere 374 kB with a very manageable 8124 rows of data. In addition the variables are single letters representing certain aspects, this helps cut down on size even further and means we don't need to run any matching or regex on the fields to find our data. All of this lead me to discard any such big data technologies from consideration.
\subsection{Problem Statement \& Data Exploration}
\subsubsection{Description}
The dataset contains a list mushrooms and describes their various characteristic, listed below, along with whether or not they are poisonous or not. The aim of this report is to build a model which will predict to degree of certainty which characteristics of the mushrooms dictate the edibility of the mushrooms.

\subsubsection{Attribute Information}

classes: edible=e, poisonous=p

cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s

cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s

cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r
,pink=p,purple=u,red=e,white=w,yellow=y

bruises: bruises=t,no=f

odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,
musty=m,none=n,pungent=p,spicy=s

gill-attachment: attached=a,descending=d,free=f,notched=n

gill-spacing: close=c,crowded=w,distant=d

gill-size: broad=b,narrow=n

gill-color: black=k,brown=n,buff=b,
chocolate=h,gray=g, green=r,orange=o,
pink=p,purple=u,red=e,white=w,yellow=y

stalk-shape: enlarging=e,tapering=t

stalk-root: bulbous=b,club=c,cup=u
,equal=e,rhizomorphs=z,rooted=r,missing=?

stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s

stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s

stalk-color-above-ring: brown=n,buff=b,cinnamon=c,
gray=g,orange=o,pink=p,red=e,white=w,yellow=y

stalk-color-below-ring: brown=n,buff=b,cinnamon=c
,gray=g,orange=o,pink=p,red=e,white=w,yellow=y

veil-type: partial=p,universal=u

veil-color: brown=n,orange=o,white=w,yellow=y

ring-number: none=n,one=o,two=t

ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,
none=n,pendant=p,sheathing=s,zone=z

spore-print-color: black=k,brown=n,buff=b,chocolate=h,
green=r,orange=o,purple=u,white=w,yellow=y

population: abundant=a,clustered=c,numerous=n,
scattered=s,several=v,solitary=y

habitat: grasses=g,leaves=l,meadows=m,paths=p,
urban=u,waste=w,woods=d
%#\footnote{taken from the data page found here \href{https://www.kaggle.com/uciml/mushroom-classification/data}{https://www.kaggle.com%
%#/uciml/mushroom-classification/data}}


\pagebreak
Loading in the dataset:
\subsubsection{Number of Rows and Columns}
<<>>=
options(scipen = 999)
df <- read.csv('mushrooms.csv',header = T)
@

\subsubsection{Number of Rows and Columns}
<<>>=
#dim(df)
cat('Number of Rows in the set is: ', nrow(df))
cat('Number of Columns/Features in the set is: ', ncol(df))
@

\subsubsection{Names of Features}
<<>>=
names(df) # names of the columns/features
@

\subsubsection{Class / Label Distribution}
<<>>=
table(df$class) # show the distribution 
@


\subsubsection{Glance at the Data}
<<>>=
dfs <- df[1:4,] # copy the first four rows into a temporary
names(dfs) <- NULL # delete the names of columns so the text output isn't massive
head(dfs) # show what the data looks like
@

\subsubsection{Distribution}
\begin{figure}[H]
\begin{center}
<<>>=
# plot the amount of edible vs. poisonous mushrooms in the set
barplot(
	table(df$class), col = c('lightblue', 'grey'), 
	names.arg = c('edible', 'poisonous'), xpd = FALSE, 
	ylim = c(3500, 4500), offset(3000), main = "Edible vs. Poisonous")
@
\caption {Amount of edible vs. poisonous in the data set}
\label{fig1}
\end {center}
\end {figure}

\subsubsection{Graphing}
<<warning=FALSE, message=FALSE>>=
library(caret)
# define the layout of the graphs
xy <- list(x=list(relation='free'), y=list(relation='free'))
@
\begin{figure}[H]
\begin{center}
<<>>=
# plot the data on a density graph
# this works by converting all of the features except the class to a number
#  then using the class as the class
featurePlot(
	sapply(df[,-c(df$class)], function (x) as.numeric(x)),
	df$class, plot='density', scales=xy, layout = c(4,6),
	pch = '|', frame = F)
@
\caption {Correlation between edible and poisonous by feature}
\label{fig2}
\end {center}
\end {figure}
As the graphs above show there is no perfect separation between the features. The best place to find any separation are the features odor, spore.print.color, ring.type, population, habitat. A number of the values show a very close relationship and this are of little consequence in judging the class such as, like veil.type, cap.shape, cap.color and gill.attachment.
\pagebreak\begin{figure}[H]
\begin{center}
<<>>=
ggplot(data = df, aes(x = spore.print.color, y = odor, color = class)) + 
	geom_jitter() + scale_color_manual(
		values = c('lightblue', 'grey')
	)
@
\caption {Showing the disparity between odor and spore.print.color}
\label{fig3}
\end {center}
\end {figure}
If we plot our two most prevalent features we can see they have very clear separation and will be the primary features our model uses to determine the class.
\pagebreak\subsection{Pre-proccessing}
<<>>=
length(unique(is.na(df))) # the length of the unique values for each col
@
Since our dataset doesn't have any missing values we don't need to worry about handling and null values or missing rows.

<<>>=
# loop through all of the features and show the amount of uniqe values
for (i in names(df)) { cat (i, ':', length(unique(df[[i]])), '\n')}
@
Because we only have one 'veil type' across all of our mushrooms we can disregard this column.
<<>>=
df <- (df[-c(17)]) # drop the veil type from the set
@

Here we will turn all of the values from letters to a numerical value
<<>>=
# run the as numeric function on each value
df <- data.frame(sapply(df, function (x) as.numeric(x)))
head(df$class, n = 1) # show that class comes out as 1 and 2
df$class <- sapply(df$class, function (x) x - 1) # make class binary
@


\section{Modelling / Classification}
I will be using the Random Forest model to handle the data as it is likely to have the highest success rate in predict whether or not the mushrooms are edible or not.

\subsection{Building Model}
\subsubsection{Divide The Dataset}
Split the dataset with 70\% for training and the remainder for testing the model
<<>>=
i <- sample(nrow(df), 0.7 * nrow(df)) # get 70% of the elements

train = df[i,] # get all the rows that match i
test =  df[-i,] # get all minus i
@
\subsubsection{Building the Model}
<<>>=
library(randomForest,warn.conflicts = FALSE)
set.seed(77) # set the seed for reproducibility

# use randomForest on our training set
forest <- randomForest(as.factor(class) ~ . ,
                          data = train, importance = T, ntree = 50)

pre <- predict(forest, test) # use our model to predict the test set
# create a new set from our prediction
sol <- data.frame(class = test$class, edible = pre)
@
\subsection{Testing and Evaluation}
<<>>=
print(forest)
@
Using all of the features of the dataset we are able to be one hundred percent accurate with our predictions.

<<>>=
table(pre == test$class) # show the amount of matches
@
\pagebreak
Quickly testing whether our test dataset matches our prediction of the test dataset, which it does.
\begin{figure}[H]
\begin{center}
%<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results =' hide', out.width= '.8\\linewidth'>>=
<<>>=
plot(forest, main = 'Error Rate For Initial Model', frame = F, col = 'blue')
@
\caption {Error rate for the initial random forest model using all features}
\label{fig4}
\end {center}
\end {figure}
The graph shows us that it takes approximately thirty tree's before the model is able to be a hundred percent accurate.

\pagebreak\subsubsection{Importance}
\begin{figure}[H]
\begin{center}
%<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results ='hide', out.width= '.8\\linewidth'>>=
<<>>=
varImpPlot(forest, main = 'Importance Rating by Feature', frame = F)
@
\caption {Importance Rating For Each Feature}
\label{fig5}
\end {center}
\end {figure}
This graph shows the importance of each of the features when predicting the edibility of the mushroom. From this graph we can see a number of features are unimportant to the model such as veil.color, gill.attachment, etc. these can effectively be ignored in the model without much or any effect on the accuracy of the model.

\pagebreak\subsubsection{Confusion Matrix}
<<>>=
#confusionMatrix(pre, test$class) # gen a confusion matrix on our prediction
@
We can see here that the accuracy is rated at 100\% which is supported by what we have seen from our prediction data when compared to the test data. From all of this we can be fairly sure in our assertion that our model is entirely accurate.

\pagebreak\section{Improving Performance}
\subsection{Shrinking The Input Features}
By looking at the graph of importance from our original model we can select the most important features
<<>>=
# reduce the amount of features we give the randomForest
forSmall <- randomForest(as.factor(class) ~ odor + gill.color + gill.size +
		spore.print.color + ring.type + stalk.root + habitat + population + bruises,
	data = train, ntree = 50)
@

We can see that this new model still maintains the same level of accuracy. It is possible to achieve the same accuracy with one less feature however on some iterations of randomForest it would return with a 0.02\% error rate
<<>>=
print(forSmall) # print out the forest so we can see the accuracy
@

And if we test this model in a prediction and compare that with our test dataset
<<>>=
table(predict(forSmall, test) == test$class) # check a prediction against the test data
@
We can see it does indeed match
\pagebreak
\begin{figure}[H]
\begin{center}
%<<fig=TRUE,warning=FALSE,message=FALSE,echo=TRUE, results='hide',out.width='.8\\linewidth'>>=
<<>>=
plot(forSmall, main = 'Error Rate With Less Features', frame = F, col = 'blue')
@
\caption {Error rate for random forest model using only the most important features}
\label{fig6}
\end {center}
\end {figure}

\subsection{Using a Conditional Tree To Compare}
\subsubsection{Train The Method}
<<>>=
#ct <- train(as.factor(class) ~ . , data = train, method ='ctree') # train using ctree
@

\subsubsection{Predict With The Model}
<<>>=
#ctpre <- predict(ct, test) # make a predeiction 
@

\subsubsection{Evaluate the accuracy}
<<>>=
#confusionMatrix(ctpre, test$class) # compare prediction to the test
#table(ctpre == test$class) # show how many are correct
@
This shows us that the the method is not 100\% accurate though the margin of error is very small at less than 1\%.

\subsection{Change Test/Train Scale}
For this section I will reduce the training size for our random forest model to see if it still maintains a high level of accuracy. If we see a fall in accuracy then we know that our original method was subject to over fitting and will need tuning.
\subsubsection{Build With Redefined Scale}
<<>>=
ismall <- sample(nrow(df), 0.1 * nrow(df)) # get 10% of the elements

trsmall = df[ismall,]
tesmall = df[-ismall,]

fsmall <- randomForest(as.factor(class) ~ . ,
                          data = trsmall, importance = T, ntree = 50)

presmall <- predict(forest, tesmall) 

solsmall <- data.frame(class = tesmall$class, edible = presmall)
@

\subsubsection{Analyse The Results}
<<>>=
print(fsmall)
#table(presmall == tesmall$class) 
@
As we can see, the model still retains it's 100\% accuracy rating even with only 10\% of the the dataset for training. This means that we aren't suffering from any over fitting.


\pagebreak
\begin{figure}
\begin{center}
%<<fig = TRUE, warning = FALSE, message = FALSE, echo = TRUE, results =' hide', out.width= '.8\\linewidth'>>=
<<>>=
plot(fsmall, main = 'Error Rate For Smaller Training Set', frame = F, col = 'blue')
@
\caption {Error rate for random forest model using a smaller training set}
\label{fig7}
\end {center}
\end {figure}
Again, we see that around thirty decision tree's are required to hit a 100\% success rate which is in line with what we saw when using a 70/30 split.

\subsection{Conclusion}
The above should show that the original model, using all of the features and a large training set may have been overkill but none the less is still flawless with this dataset. It does not suffer from over fitting or edge cases throwing it out. 
I believe that random forest was the right choice of model for this dataset, as it is able to reliably be 100\% accurate even while restricting the inputs somewhat and it beats the correlation tree by comparison, and though the margin here is small, since we are dealing with potentially serious or fatal poisons here the 0.02\% is pretty undesirable.
\end{document}
